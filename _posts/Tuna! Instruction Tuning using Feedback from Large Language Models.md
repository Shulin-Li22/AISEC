---
layout: single  # 使用 single 布局
title: "Tuna: Instruction Tuning using Feedback from Large Language Models"
date: 2025-11-07

# (可选) 添加标签和分类
tags:
  - Blog
  - LLM_Security
  - Instruction Tuning
categories:
  - LLM_Security

# (可选) 摘要，会显示在主页的文章列表里
excerpt: "本文提出了一种利用新颖的概率排序和上下文排序方法来微调指令调整后的 LLM，甚至可以获得比几个强大的强化学习基线更好的结果。"
---

# Overview

这篇文章主要介绍了一种名为 **Tuna** 的新方法，旨在通过“指令调优”（Instruction Tuning）来提升开源大型语言模型（LLMs）的性能。

研究人员指出，现有的指令调优方法（如Alpaca）虽然有效，但通常只让模型学习由更强模型（如GPT-4）生成的**单一**回复。这种方式的缺陷在于，模型没有学习到为什么某些回复可能优于其他回复。



为了解决这个问题，该论文提出了两种新的精调（finetuning）方法：

1. **概率排名 (Probabilistic Ranking)**

   * 此方法利用“教师”LLM（如 text-davinci-003）的能力。

   * 它首先让教师LLM为同一条指令生成*多个*不同的回复。

   * 然后，它根据教师LLM评估这些回复的概率（即模型认为哪个回复更“好”）对它们进行排序。

   * 最后，训练学生模型（如LLaMA）来学习这个“相对排名”，使其能够区分高质量和低质量的回复。

2. **上下文排名 (Contextual Ranking)**&#x20;

   * 此方法旨在利用更强LLM（如 GPT-4）的上下文理解能力，来优化模型*自己*生成的回复。

   * 它首先从*学生模型*（即正在被训练的模型）那里采样N个不同的回复。

   * 接着，它利用GPT-4（通过特定的提示）来评估这N个回复，并给出一个综合排名。

   * 学生模型随后学习这个由GPT-4提供的排名，从而优化自己的回复分布，缓解“暴露偏差”（exposure bias）问题。



## 什么是 Tuna 模型？

最终的 **Tuna** 模型是将这两种方法**顺序**应用在已经经过指令调优的LLM（如Alpaca）上得到的结果。

* **流程**：(基础LLM) -> (标准指令调优，如Alpaca) -> (概率排名) -> (上下文排名) -> **Tuna**



## 结论

实验结果表明，Tuna模型在多个基准测试（包括Super Natural Instructions, LMentry和Vicuna QA）上的表现**显著优于**标准的Alpaca模型，并且甚至超过了一些强大的强化学习（RLHF）基线模型。



# Methodology

好的，这篇文章的方法论（Methodology）部分（第2节）详细介绍了Tuna模型是如何构建的，它建立在标准的“指令调优”之上，并引入了两种新的排名方法 (1)。

以下是其方法论的分解：

## 1. 基准：指令调优 (Instruction Tuning)

* **目标**：首先，文章回顾了标准的指令调优。这是为了让预训练的LLM（通常只擅长“下一个词预测”）转变为能遵循人类指令的、有帮助的助手。

* **方法**：这一步通过在“指令-回复”数据对上以监督学习的方式精调模型来实现。

* **数据来源**：这些数据对可以由人类标注，或者（更常见且成本更低）由像GPT-4这样的强LLM生成，例如使用Self-Instruct算法。



## 2. 概率排名 (Probabilistic Ranking)

这是Tuna的第一个核心创新，旨在让模型学会从“教师”LLM的角度区分好坏回复。

* **步骤1：生成多份回复**

  * 放弃只学习*一个*回复的传统方式，研究者使用一个强的“教师”LLM（如 text-davinci-003）为数据集中的每条指令生成N个（例如4个）新的回复。

* **步骤2：教师模型打分与排名**

  * 教师LLM虽然强大，但它生成的N个回复质量也会有波动。

  * 研究者使用教师LLM给出的“规范化对数似然”（normalized log-likelihood）作为质量分数。这个分数考虑了回复的长度（使用长度惩罚$$\beta$$），以确保公平比较。

  * 根据这个分数 $$s(i,r^{(k)})$$ 对N个回复进行降序排序。

* **步骤3：学习排名**

  * 学生模型（如Alpaca）使用一个**成对排名目标函数（pairwise ranking objective）** 来学习这个顺序。

  * 目标是让模型赋予排名靠前（$$r^{[j]}$$）的回复比排名靠后（$$r^{[k]}$$）的回复更高的概率（或得分 $$v_{\theta^{\prime}}$$） 。

* **步骤4：结合正则化**

  * 为了防止模型在排名任务上“过拟合”而忘记了原始的指令跟随能力，他们还将这个排名损失（$$L_{rank}$$）与原始的标准指令调优损失（$$L_{MLE}$$）结合起来。



## 3. 上下文排名 (Contextual Ranking)

这是第二个核心创新，旨在利用更强LLM（如GPT-4）的上下文理解能力，来优化模型*自己*的回复分布，以缓解“暴露偏差”（exposure bias）问题。



* **步骤1：从学生模型采样**

  * 这次不是从教师模型生成回复，而是让*学生模型自己*（即正在训练的Alpaca模型）为每条指令生成N个回复。

  * 为了确保回复的多样性，他们使用了一个技巧：如果两个回复过于相似（通过ROUGE-L分数判断），就会提高采样温度并重新采样。

* **步骤2：GPT-4评估与排名**

  * 接着，他们利用GPT-4的强大上下文理解能力来对这N个回复进行排序。

  * 这个过程很详细：GPT-4首先判断指令是“开放式”还是“封闭式” ，然后自己生成一个“参考答案” 。

  * **开放式问题**：GPT-4根据相关性、细节水平和准确性打分。

  * **封闭式问题**：GPT-4根据准确性、细节水平和清晰度打分。

  * GPT-4最后根据总分对这N个回复进行排序。

* **步骤3：学习排名**

  * 与概率排名类似，模型使用相同的目标函数（公式6）来学习这个由GPT-4提供的排名。



## 4. 整合：Tuna 模型的最终流程

文章探讨了如何组合这两种方法，并最终确定了Tuna模型的训练流程：

1. 首先，在标准指令调优模型（如Alpaca）上应用**概率排名**（得到模型 $$Tuna_p$$）。

2. 然后，在 $$Tuna_p$$ 模型的基础上，再应用**上下文排名**。

3. 最终得到的模型就是 **Tuna**。

研究者认为这个顺序是最好的，因为概率排名首先让模型学习教师LLM的知识，而随后的上下文排名则帮助模型根据*自身*的能力（而非教师模型的能力）来优化其回复，从而更有效地缓解暴露偏差。



# Experiments & Results

## 1. 实验设置

### 模型与数据 (Model and Data)&#x20;

* **基础模型**：他们使用了 70亿（7B）参数的 LLaMA 模型作为所有实验的基础。

* **基线模型 (Alpaca)**：他们首先使用 Alpaca 的 52K 指令数据对 LLaMA 进行了标准的指令调优，这个调优后的模型被称为 Alpaca 。

* **Tuna\_p (概率排名)**：使用 `text-davinci-003` 为 52K 的 Alpaca 指令生成了N=4个回复，并基于概率进行排名，然后用这些数据精调 Alpaca 模型。

* **Tuna\_c (上下文排名)**：让 *Alpaca 模型自己* 为 13K 条指令生成N=4个回复，然后使用 **GPT-4** 来对这些回复进行排名，再用这些数据精调 Alpaca。

* **Tuna (最终模型)**：这是他们的最终方案。他们首先训练出 $$Tuna_p$$ 模型，然后让 $$Tuna_p$$ 模型生成回复，再请 **GPT-4** 对 $$Tuna_p$$ 的回复进行上下文排名（使用13K数据），最后精调 $$Tuna_p$$ 得到最终的 Tuna 模型。

* **对比的RLHF基线**：他们还引入了两个强大的RLHF（强化学习）模型（PPO-sim 和 PPO-sim-GPT4-20K）作为对比。



### 评估基准 (Evaluation)&#x20;

研究人员在三个主要基准上进行了评估：

1. **Super Natural Instruction (Super NI)**：包含119个不同的测试任务，用于评估模型的跨任务泛化能力。使用 ROUGE-L 分数进行评估。

2. **LMentry**：包含25个任务，专注于测试模型生成内容的准确性和鲁棒性 。

3. **Vicuna QA**：包含80个开放式问题。由于没有标准答案，他们使用 **GPT-4** 进行成对比较，判断哪个模型的回答更好，并报告“赢/输/平”的比例。

4. **Human Evaluation**：他们还在 Vicuna QA 上进行了人工评估，让人类标注者对5个匿名模型的回复进行排名。



## 2. 主要实验结果

### 主结果 (Main Results)&#x20;

* **$$Tuna_p$$ 和&#x20;**$$Tuna_c$$**&#x20;均优于基线**：

  * 与标准的 Alpaca 模型相比，单独使用概率排名 ($$Tuna_p$$) 或上下文排名 ($$Tuna_c$$) 都能在三个基准测试上带来性能提升。

  * $$Tuna_p$$ 在 Super NI (0-shot) 和 LMentry（准确性任务）上表现更好。

  * $$Tuna_c$$ 在 Super NI (2-shot)（长序列任务）上表现更优。

* **与RLHF基线对比**：

  * 在 Super NI 和 LMentry（准确性）上，$$Tuna_p$$ 和 $$Tuna_c$$ 始终优于两个强大的 PPO (RLHF) 基线。

  * 但在 Vicuna QA（开放式问答）上，它们的表现略低于 RLHF 基线。

* **Tuna (最终模型) 表现最佳**：

  * Tuna 模型（即 $$Tuna_p$$ + 上下文排名）在保持 Super NI 和 LMentry 竞争力的同时，**在 Vicuna QA 上取得了最好的表现**。

  * **人类评估（Table 2）证实了这一点**：Tuna 获得了最高的人类评分（3.80），显著优于 Alpaca (2.13) 和其他所有模型。



### 消融实验和关键发现

研究人员还进行了一系列“消融研究” (Ablation Study) 来探究为什么他们的方法有效。

* **发现1：Tuna的提升不仅仅是因为数据更多。**

  * 他们尝试了一个叫 Alpaca-Mul 的模型，即在标准指令调优时为每条指令简单地增加4个回复（但不进行排名学习）。

  * 结果发现，Alpaca-Mul 的性能几乎没有提升，甚至在 Vicuna QA 上还不如原版 Alpaca。

  * 这证明了 Tuna 的成功来自于**学习排名（区分好坏）**，而不仅仅是看了更多的回复。

* **发现2：训练顺序很重要。**

  * 他们尝试了相反的顺序，即先用上下文排名，再用概率排名（称为 $$Tuna_{cp}$$）。

  * 结果发现，$$Tuna_{cp}$$ 的表现不如 Tuna。

  * 这表明先进行概率排名（学习教师模型的知识），再进行上下文排名（优化自身分布）的顺序是更优的。

* **发现3：概率排名不需要全部数据。**

  * 他们发现，使用概率排名时，数据量在13K到24K时性能就已达到饱和。这意味着研究人员可以将数据生成成本降低一半。

* **发现4：GPT-4 评估存在风险（偏见）。**

  * 这是一个非常有趣的发现。他们发现 GPT-4 在进行“上下文排名”时，表现出了**对更长回复的偏好**。

  * 他们训练了一个代理排名模型（PRM）来模仿GPT-4的排名行为。这个PRM模型在 Vicuna QA（由GPT-4评估）上表现尚可，但在 Super NI 和 LMentry（由ROUGE-L评估）上表现很差。

  * **结论**：这表明完全依赖GPT-4来评估开放式问答可能是有风险的，因为它可能偏爱更长、更详细但*不一定*更准确的答案。
