---
layout: single  # 使用 single 布局
title: "InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning"
date: 2025-12-30

# (可选) 添加标签和分类
tags:
  - Blog
  - LLM_Security
  - Multimodal
  - Multimodal Contrastive Learning
  - Backdoor Defense
categories:
  - LLM_Security
  - Backdoor Defense
  - Multimodal
  - Multimodal Contrastive Learning

# (可选) 摘要，会显示在主页的文章列表里
excerpt: "这篇论文提出一种名为 InverTune 的防御框架，通过对抗扰动识别目标标签、梯度优化逆向重建触发器并结合激活聚类进行定向微调，以极低的数据开销实现了对多模态对比学习模型后门的高效移除与原有性能的精准保持 。"
---


## Introduction

### 1. 背景与挑战

* **MCL 的兴起**：以 CLIP 为代表的模型通过海量的互联网图像-文本对进行预训练，实现了卓越的视觉-语言对齐能力，在零样本分类、图像字幕生成等任务中表现优异 。


* **安全威胁**：由于这些模型高度依赖从网络爬取的训练数据，攻击者可以轻易植入“潜伏”的**后门触发器** 。一旦触发器出现，模型就会按攻击者的意图产生错误输出（例如将任意图片识别为特定标签） 。


* **防御难点**：
    * **开放词汇限制**：传统单模态模型的标签空间是预定义的、离散的，可以通过遍历来识别攻击目标 ；但 MCL 模型具有“开放词汇”特性，无法穷举 。


    * **现有防御的缺陷**：现有的防御机制通常假设防御者拥有攻击者的先验知识，或者需要大量的干净数据进行重新训练 。此外，它们往往在“防御效果”与“模型性能（干净数据的准确率）”之间难以达到理想的平衡 。





---

### 2. InverTune 的核心思路

InverTune 被定位为首个在**极低攻击者假设**（不需要知道攻击目标、不接触中毒数据集）下工作的多模态模型后门防御框架 。其防御过程分为三个关键步骤：

* **步骤一：目标识别（Target Identification）**
防御者发现，受攻击的模型对“对抗性扰动”表现出独特的脆弱性 。通过分析模型在对抗攻击下的响应模式，InverTune 能准确识别出攻击者设定的目标标签 。


* **步骤二：触发器逆向（Trigger Inversion）**
利用梯度逆向技术和激活模式分析，在不接触原始毒数据的情况下，重建出模型内部隐藏的触发器特征 。


* **步骤三：激活调优（Activation Tuning）**
通过聚类分析找到与后门相关的神经元，并在少量干净数据上进行选择性微调 。这样可以精准消除后门功能，同时最大限度地保留模型原有的对齐能力 。



---

### 3. 主要研究成果与贡献

论文强调了 InverTune 的卓越表现：

* **高效性**：将平均攻击成功率（ASR）降低了 **97.87%** 

* **低开销**：所需的干净数据量仅为现有主流方法的 **1/10** 。

* **高保真**：对模型正常任务的准确率（CA）影响极小，下降仅约为 **3.07%** 。

* **首创性**：该研究首次实现了对 MCL 模型中后门目标标签的直接识别 。


## Related Work

### 1 多模态对比学习中的后门攻击

* **传统攻击的演变**：传统的后门攻击（如 BadNet, Blended, SIG 和 TrojanNet）最初针对单模态网络，但可以通过数据中毒手段适配到 MCL 模型中 。

* **MCL 特定攻击**：近年来的研究发现，专门针对 MCL 的攻击能更有效地利用跨模态交互 。

* **低中毒率威胁**：研究证明，极少量的中毒数据就能在模型中引入严重的漏洞 。


* **针对编码器的攻击**：例如 **BadEncoder** 针对自监督学习中的预训练图像编码器，使得下游分类器继承后门行为 ；**GhostEncoder** 则利用图像隐写技术嵌入动态且不可见的隐藏触发器 。


* **最先进的攻击手段**：**BadCLIP** 引入了双嵌入框架，使中毒样本与目标特征对齐，生成的触发器更加自然且难以被标准防御手段识别 ；此外，还有通过可学习触发器和提示词（Prompts）来同时操纵图像和文本编码器的攻击方法 。



### 2 多模态对比学习中的后门防御

* **防御分类**：MCL 的防御策略主要分为检测（Detection）和缓解（Mitigation/Removal）两类 。


* **检测与净化的局限性**：**DECREE** 专注于识别后门但缺乏移除机制 ；**SSL-Cleanse** 虽然包含净化过程，但主要针对自监督学习设计 。


* **基于微调的防御**：如 **CleanCLIP**、**PAR** 和 **CleanerCLIP**，尝试通过重新学习表示或反事实增强来移除后门 。然而，这些方法往往需要庞大的干净数据集，或者在安全性与模型性能之间存在严重的权衡问题 。


* **对抗性防御尝试**：**ABD** 方法创造性地利用对抗样本来近似后门样本，但在维持模型在干净数据上的准确率（Clean Accuracy）方面面临挑战 。


* **预训练阶段的防御**：例如 **RoCLIP** 和 **SafeCLIP**，通过在预训练期间过滤中毒数据来缓解后门 。但这类方法必须能够访问预训练过程，因此不适用于防御者仅拥有已训练好的模型（即现成模型）的情况 。


* **InverTune 的定位**：相比之下，本研究提出的 InverTune 旨在有效消除后门威胁，同时在不需要访问预训练过程的情况下，最大限度地保留模型的原始性能和泛化能力 。


## Threat Model
论文的第二部分 **“Threat Model”（威胁模型）** 详细定义了攻击者和防御者的角色、能力假设以及各自的目标。

### 1. 攻击者 (Attacker)

* **攻击目标**：攻击者针对多模态对比学习（MCL）模型（如 CLIP）的**视觉编码器** 。


* **能力假设**：
    * 攻击者可以构建用于模型微调的中毒数据集 。

    * 攻击者了解目标模型的架构和具体参数 。


* **攻击目的**：在预训练的 CLIP 模型中植入后门。模型在处理良性输入时表现正常，但一旦输入带有特定触发器（Trigger）的内容，就会产生错误的输出 。


* **攻击手段**：攻击者在微调数据集中注入一小部分中毒样本，这些样本带有视觉触发器 。通过在该中毒数据集上进行微调，攻击者能够操纵模型对视觉触发器的反应方式 。


* **攻击范围限制**：一旦视觉编码器被植入后门并部署，攻击者无法进一步控制下游的具体应用或任务 。



### 2. 防御者 (Defender)

* **基本假设**：
* 防御者无法访问原始的预训练数据集，也无法获得攻击者使用的中毒微调数据集 。


* 防御者完全不知道后门攻击的具体目标（即不知道触发器是什么，也不知道目标标签是什么） 。


* **数据限制**：防御者没有完整的干净数据集，或者仅能接触到**极少量的干净数据** 。


* **防御目的**：在不破坏模型在干净数据上原有性能的前提下，中和并消除隐藏的后门 。



# INVERTUNE: DETAILED CONSTRUCTION”

InverTune 的防御过程由三个关键步骤组成：基于对抗扰动的目标识别、触发器逆向以及基于激活聚类的微调 。

### 1 目标识别 (Target Identification)

这一步旨在识别攻击者设定的后门目标标签 。

* **核心观察**：作者发现后门模型在受到对抗性扰动（Adversarial Perturbations）时会表现出独特的行为 。与干净模型不同，后门模型往往会将受到扰动的样本不成比例地分类到“目标标签”下 。


* **识别策略**：防御者会构建一个通用的对抗扰动（Universal Adversarial Perturbation, UAP），旨在诱导模型发生系统性的误分类 。通过对比模型在对抗扰动样本上的输出分布（$P_{adv}(y)$）与在干净样本上的分布（$P_{clean}(y)$），将预测频率增加幅度最大的类别确定为目标标签  。这一策略大幅降低了在开放词汇环境下穷举搜索的计算开销 。



### 3.2 触发器逆向 (Trigger Inversion)

在确定目标标签后，防御者需要重建出潜伏的触发器 。

* **多模态挑战**：由于 CLIP 模型在共享的高维嵌入空间中操作，传统的单模态逆向方法无法直接应用 。


* **双空间优化**：InverTune 提出了一种同时考虑视觉嵌入空间和跨模态对齐的“双空间”优化方案 。它将触发器参数化为“掩码-图案”对（$m,t_{img}$），并利用以下四个协同损失函数进行优化 ：

    * **跨模态对齐损失 ($L_{\text{align}}$)**：强制视觉触发器嵌入与目标文本对齐 。


    * **嵌入空间保护损失 ($L_{\text{emb}}$)**：防止触发器样本过度偏离原始分布，以保护模型的泛化能力 。


    * **视觉相似性损失 ($L_{\text{sim}}$)**：确保触发器在视觉上尽可能隐蔽 。


    * **触发器稀疏性损失 ($L_{\text{mask}}$)**：限制触发器的面积，使其更符合攻击者的实际操作 。




### 3.3 激活调优 (Activation Tuning)

最后一步是消除后门，同时保留模型的正常功能 。

* **层与神经元选择**：
1. **关键层选择**：通过量化干净输入和触发器输入之间的“标准化激活差异”（Activation Divergence），识别出对触发器反应最剧烈的网络层（通常是较深层）。


2. **关键神经元识别**：在选定层内使用 **K-means 聚类** 分析神经元的响应模式，精准找出负责后门功能的特定神经元簇 。


3. **选择性微调**：
* 使用 **激活对齐损失 ($L_{\text{activation}}$)** 强制这些关键神经元对干净样本和触发器样本表现出相似的激活模式，从而抑制恶意激活峰值 。


* 同时引入 **跨模态一致性损失 ($L_{\text{preserve}}$)**，确保微调后的模型在正常任务上的表现与原始模型保持一致 。


* **精准更新**：在微调过程中应用“神经元掩码”（Neuron Masks），仅允许更新识别出的关键神经元参数，从而最大限度地减少对模型整体性能的干扰 。


## Experiments & Results

### 1. 实验设置

* **模型与架构**：默认使用 OpenAI 的 CLIP (RN50) 作为基础模型 。为了验证通用性，实验还扩展到了 RN101、ViT-B/16 和 ViT-B/32 架构 。


* **数据集**：使用 CC3M 的 50 万个子集进行中毒微调 。评估任务包括 ImageNet-1K 的零样本分类和 MSCOCO 的图像-文本检索 。


* **攻击方法**：涵盖了 6 种攻击，包括 4 种单模态攻击（BadNet, Blended, SIG, WaNet）、1 种自监督攻击（BadEncoder）以及最先进的 CLIP 专用攻击 BadCLIP 。


* **数据需求**：InverTune 仅需 5 万张干净图片进行逆向，而在最终的激活调优阶段，仅需 **1 个 Batch（64张）** 的任意干净数据 。相比之下，其他基线方法需要的数据量是其 10 倍 。

---

### 2. 防御性能对比

实验结果显示，InverTune 在降低攻击成功率（ASR）的同时，极好地保留了干净准确率（CA）：

| 任务 | 攻击类型 (以 BadCLIP 为例) | 无防御 ASR | InverTune 防御后 ASR | CA (准确率) 损耗 |
| --- | --- | --- | --- | --- |
| **图像分类** | BadCLIP (ImageNet) | 98.36%  | **0.49%**  | 仅下降约 3.07%  |
| **检索任务** | BadCLIP (MSCOCO) | 99.28%  | **0.68%**  | 保持在 69.58%  |

* **核心优势**：在对抗最先进的 BadCLIP 攻击时，现有基线方法（如 PAR）仍残留超过 15% 的 ASR，而 InverTune 能将其彻底压制到 1% 以下 。



### 3. 关键步骤验证 (Step Verification)

* **目标识别有效性**：在以“蘑菇”为目标的模型中，应用扰动后，“蘑菇”类的分类频率激增了 97.23%，证明了第一步识别的精准度 。


* **逆向触发器效果**：实验证明，逆向生成的触发器能够成功模拟原始攻击行为，激活后门路径 。


* **消融实验 (Ablation Study)**：对比了“直接使用对抗扰动微调”与“使用逆向触发器微调” 。结果显示，只有通过 InverTune 的**触发器逆向**步骤，才能触及深层的后门信息，否则 ASR 依然很高 。



### 4. 稳健性分析 (Robustness Analysis)

* **目标标签的影响**：无论攻击目标是“香蕉”、“柠檬”还是“滑雪”，InverTune 都能稳定地将 ASR 降至 1% 左右，而基线方法在不同标签下的效果波动极大 。


* **架构通用性**：InverTune 在 ResNet（基于 CNN）和 ViT（基于 Transformer）架构上表现一致 。


* **发现**：ResNet 的后门敏感性集中在最后一层 ；而 ViT 的敏感性分布在中间层（如第 3-5 个 Block）。



### 5. 超参数敏感性 (Hyperparameter Sensitivity)

* **参数平衡**： 控制“后门移除”与“性能保留”之间的平衡 。

* **最佳实践**：实验发现，当  时，模型能在维持极低 ASR 的同时，最大化保留 CA 。



## Conclusion

### 总结

* **核心框架总结**：论文提出了一种名为 **InverTune** 的新型后门防御框架，专门用于大规模多模态对比学习（MCL）模型 。


* **三大技术支柱**：该框架集成了三个关键组件：基于对抗攻击的目标标签识别、梯度引导的触发器逆向、以及基于激活感知的微调 。


* **性能表现**：广泛的评估证明，InverTune 在各种攻击场景下均达到了最先进的（SOTA）防御水平 。它能持续降低攻击成功率（ASR），同时极好地保留了模型的原有效用 。


* **实际意义**：该研究显著增强了多模态模型对抗后门威胁的稳健性，为现实世界的应用部署提供了一种实用的解决方案 。


### 局限性

作者在文末也提到了三个主要的局限性，指明了未来的研究方向：

* **模型覆盖范围**：目前的研究主要集中在 **CLIP 框架**及其多种架构（如 RN50, ViT 等）上 。作者尚未探索其他大规模多模态学习架构，这些架构可能表现出不同的漏洞或防御特征 。

* **对抗样本生成技术**：在识别目标标签时，本研究主要依赖于 **AdvCLIP** 提供的方法 。作者认为，未来可以尝试更多种类的对抗性生成技术，以更全面地了解它们与多模态模型的交互方式 。

* **广度探索**：未来的工作可以进一步扩大研究范围，提供关于多模态架构和对抗方法的更全面的视角 。